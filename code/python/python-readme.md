# Python readme

plinko_env.yml specifies the package requirements for running the python code and can be used to construct a python environment.

Simulation models in both the prediction and inference task rely on code provided on the top level.

1. `engine.py` contains the noisy simulator used to model intuitive physical mental simulation in both tasks.

2. `visual.py` contains code for visualizing the Plinko box.

3. `convert_coordinate.py` contains code to convert from the pymunk physics engine coordinate system to the unity rendered Plinko box coordinate system.

4. `config.py` contains code to generate initial specifications for Plinko trials.

5. `utils.py` contains general utilities for loading and generating Plinko worlds.

The bulk of the model code and analysis are included for the two experiments here. Model behavior is pre-generated and provided in the repo so that analysis can be run directly. Alternatively model behavior can be regenerated from scratch using instructions provided.

## Prediction

Code to generate performance file for the IPE as well as the cross-validation model comparison are contained in the `simulator_model_comparison.ipynb` notebook. Run through the full analysis and generate model behavior for downstream visualizations by running the notebook from top to bottom.

`simulator_model_comparison.ipynb` relies on IPE simulations for different parameter settings previously generated and saved in `grid_search`. These simulations are precomputed in the repo, but can be regenerated by running the `generate_simulations.ipynb` notebook from top to bottom.

`simulator_model_comparison.ipynb` also relies on previously generated model behavior from the three statistical models. These in turn rely on pretraining data. Both are provided precomputed, but to generate from scratch, first run the `world_generator.ipynb` notebook which creates the pretraining train and test worlds for the statistical models, and then run each of the `cvsplits-<model_name>_w_pretraining.ipynb` notbooks for each of the linear, mlp, and cnn models. `cvsplits-cnn_w_pretraining.ipynb` may require a gpu to run in a reasonable amount of time.

`visualizations.ipynb` can be used to generate figure subcomponents in Figure 2 A, B, C, and the appendix prediction visualizations. Prediction scatter plot is generated in the R code.

## Inference

### Using Pre-computed Behavior

`analysis.ipynb` contains the primary anaylis for the inference task, including the grid search to determine the optimal sequential and uniform samplers, and the inference cross validation. As in the prediction task, model behavior for the grid of parameter settings is precomputed and provided the `model_performance` folder. `model_performance` contains multiple subdirectories:

1. `judgment_rt` is the save location for model judgment behavior that is compared against participant judgments.

2. `collisions` is the save location for the records of model simulation behavior which are used to generate eye-data predictions. Because these files take up substantial memory, we only provide the collisions files for the top performing models which are used for paper visualizations.

3. `emd` is the save location for the records of Earth Movers Distance between model regression predictions and participant gaze distributions.

4. `top_models` contains the judgement and eye-data emd performance for the top sequential sampler and uniform sampler in each condition. These files are used to generate results graphs in R which show up in the main paper results.

5. `cross_validation` contains the Earth Movers Distance records for all the paramater settings in all the cross-validation splits. Because the cross-validation training sets involve a subset of the total trials, model eye-data predictions need to be refit on this subset in cross-validation.

### Regenerate from Scratch

`data_cleaning.ipynb` contains code for the initial cleaning of the data set, removing response time outliers.

Regenerating model behavior from scratch then requires several steps. First IPE simulations can be pre-generated to speed up computatation. `generate_simulations.ipynb` performs this task. Run this notebook from top to bottom to generate a compressed gz simulation record in `saved_sims`. The notebook is preset with the optimal physics parameters fit in the prediction experiment.

Generating model predictions for the eye-data also requires pre-computed heatmaps representing visual features of the trials and heatmaps of participant gaze. To generate these heatmaps, run the `heatmap_setup.ipynb` notebook from top to bottom. This step will also be required to run `visualizations.ipynb` and generate the subcomponents of the paper figures for the inference task.

Next model behavior for the Sequential Sampler or Uniform Sampler can be generated. We distributed these computations on Stanford's high performance compute cluster Sherlock. Model behavior for a individual parameter settings can be re-generated using `run_model.py` and `run_regression.py`. First model judgment and simulation behavior are generated with `run_model.py` with parameters provided on the command line. For example, for the Sequential Sampler, enter the following:
```
% python run_model.py sequential <evidence_type> proportional 100 <vision_bw> <audio_bw> uniform uniform 0.3 0.0 0.7 
```
The given terms represent fixed parameters for this work. The three variables are evidence_type, vision_bw, and audio_bw. evidence_type can take on the values vision_independent (No Sound + Ball Visible model), vision_sound_independent (Sound + Ball Visible model), or sound_independent (Sound + Ball Occluded Model). vision_bw can be any integer, and audio_bw can be any float. When the evidence_type is vision_independent, only the vision_bw should be provided. When the evidence_type is sound_independent, only the audio_bw should be provided. When the evidence_type is vision_sound_independent, both bws should be provided.

Generating behavior for the Uniform Sampler is similar, but with fewer added fixed parameters. For example:
```
% python run_model.py uniform <evidence_type> 100 <vision_bw> <audio_bw> 0.3 0.0 0.7
```
The same coordination between evidence_type and the provision of the bandwidths is required here.

Running this code will generate judgment behavior in `model_performance/judgment_rt` as well as simulation behavior in `model_performance/collisions`. This simulation behavior can then be used to generate eye-data distribution distances with `run_regression.py`. The command line syntax for `run_regression.py` is similar with a couple additional parameters. For example, for the Sequential Sampler enter:
```
% python run_regression.py sequential <evidence_type> proportional 100 <vision_bw> <audio_bw> uniform uniform 0.3 0.0 0.7 <condition> 300
```
Here condition indicates the behavioral condition, the code scheme vision, audio, and occluded corresponding to "No Sound + Ball Visible", "Sound + Ball Visible", and "Sound + Ball Occluded" respectively. Similarly for the Uniform Sampler enter:
```
% python run_regression.py uniform <evidence_type> 100 <vision_bw> <audio_bw> 0.3 0.0 0.7 <condition> 300
```

Running this code generates Earth Movers Distance record between the model prediction and the distribution of participant gaze in `model_performance/emd`.

EMD performance can also be generated for the visual features model and the naive baseline by running the `regression_alternative_models.ipynb` from top to bottom. 

To generate model predictions for the eye-data across splits in the cross-validation, regressions need to be refit with the subset of trials appropriate to the given split. `cross_val_reg.py` breaks down a fixed grid multiplied across splits into chunks of parameter settings that will be run in sequence on a single job. The grid is specified in the file. `cross_val_reg.py` requires the model_type (sequential or uniform), the task id, and the total number of jobs running to determine how to break up the grid into chunks. This code is intended to operate on a compute cluster, but could in principle be run in sequence on a single machine with task_id=1 and job_id=1.

Finally, to generate visualization subcomponents in Figure 3, 4, and 5, run `visualizations.ipynb` from top to bottom. Results graphs in Figure 6 are generated using R.